{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044d6f5c",
   "metadata": {},
   "source": [
    "# Step 1 :-  Prepare Training Data\n",
    "The first step in building any tokenizer is to have a corpus of text to train it on.The Tokenizer learns *merge rules* based on the frequency of the character pairs in the data.\n",
    "\n",
    "1. i : 1\n",
    "2. s: 2\n",
    "3. is : 3\n",
    "\n",
    "Even thogh \"i\" and \"s\" are seperate tokens, we create a new token \"is\" by merging them as thy frequently appear together (is, this, miss, dismiss, list, fist, twist etc) reducing computations by 2x where we merge those two tokens \"i\" and \"s\". This is how we will iteratively merge most frequent pairs. The  new tokens can also be merged further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9880ac",
   "metadata": {},
   "source": [
    "#### training corpus:\n",
    "#### Category: Conversational\n",
    "---\n",
    "User: Hey, how's it going?\n",
    "Bot: I'm doing well! How about you?\n",
    "User: Pretty good, just learning about AI.\n",
    "Bot: That sounds fascinating! What aspects interest you the most?\n",
    "\n",
    "#### Category: Technical\n",
    "---\n",
    "Sentence: Tokenization breaks text into smaller units called tokens.\n",
    "Sentence: Embeddings represent words as numerical vectors in a multidimensional space.\n",
    "Sentence: Sampling methods influence how a language model selects its next word.\n",
    "\n",
    "#### Category: Storytelling\n",
    "---\n",
    "Once upon a time, in a distant galaxy, a lone explorer embarked on a mission to uncover the secrets of an ancient civilization. As they decoded mysterious symbols, the truth about their origins slowly unfolded…\n",
    "\n",
    "#### Category: Formal Writing\n",
    "---\n",
    "Dear Hiring Manager,  \n",
    "I am writing to express my interest in the Data Science position at your company. With a background in machine learning and AI research, I believe my experience aligns well with the role. I look forward to the opportunity to contribute to your team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0003947",
   "metadata": {},
   "source": [
    "# Step - 2: initalize vocabulary and pre-tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0a6e1",
   "metadata": {},
   "source": [
    "The BPE algorithm starts with a base vocab consisting of all unique chracters present in the training data.\n",
    " \n",
    "We also need to pre-tokenize the corpus. This usually involves splitting the text into words (or words like units), and then representing each word as a sequence of its individual characters. We often add a end-of-word token(</w>)\n",
    "to mark word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252a4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [ \"Once upon a time, in a distant galaxy, a lone explorer embarked on a mission to uncover the secrets of an ancient civilization. As they decoded mysterious symbols, the truth about their origins slowly unfolded…\",\n",
    "              \"In a world where technology and magic coexisted, a young sorcerer discovered a hidden power within themselves. With the help of an unlikely ally, they set out to master their abilities and confront an impending darkness that threatened their realm.\",\n",
    "                \"In a post-apocalyptic wasteland, a group of survivors banded together to rebuild society. As they faced external threats and internal conflicts, they learned the true meaning of community and resilience.\",\n",
    "                \"In a bustling metropolis, a detective with a troubled past found themselves entangled in a web of crime and corruption. As they navigated the shadows of the city, they uncovered a conspiracy that reached the highest echelons of power.\",\n",
    "                \"In a small town, a mysterious stranger arrived, bringing with them tales of adventure and intrigue. As the townsfolk became embroiled in their stories, they discovered hidden truths about themselves and their community.\",\n",
    "                \"In a fantasy realm, a young hero set out on a quest to retrieve a stolen artifact that held the key to restoring balance to their world. Along the way, they encountered mythical creatures, treacherous landscapes, and unexpected allies.\",\n",
    "                \"In a dystopian future, a group of rebels fought against an oppressive regime. As they risked everything for freedom, they discovered the power of hope and the strength of the human spirit.\",\n",
    "                \"In a historical setting.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd2d62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "u\n",
      "p\n",
      "o\n",
      "a\n",
      "t\n",
      "i\n",
      "m\n",
      ",\n",
      "d\n",
      "s\n",
      "g\n",
      "l\n",
      "x\n",
      "y\n",
      "r\n",
      "b\n",
      "k\n",
      "v\n",
      "h\n",
      "f\n",
      "z\n",
      ".\n",
      "A\n",
      "w\n",
      "…\n",
      "I\n",
      "W\n",
      "-\n",
      "q\n"
     ]
    }
   ],
   "source": [
    "unique_char = set()  # set does not allow duplicates\n",
    "# Iterate through each sentence in the corpus\n",
    "for sentence in corpus:\n",
    "    # Iterate through each character in the sentence\n",
    "    for char in sentence:\n",
    "        # Check if the character is not already in the set\n",
    "        if char not in unique_char:\n",
    "            # If not, add it to the set\n",
    "            print(char)\n",
    "            # Add the character to the set\n",
    "        unique_char.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ab66b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ',', '-', '.', 'A', 'I', 'O', 'W', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '…', '</w>']\n",
      "len of vocabulary:  35\n"
     ]
    }
   ],
   "source": [
    "vocab = list(unique_char)  # Convert the set to a list\n",
    "vocab.sort()  # sort the list\n",
    "\n",
    "# add end of word char\n",
    "vocab.append('</w>')\n",
    "\n",
    "print(vocab)\n",
    "print(\"len of vocabulary: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37152c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_word = vocab[34]  # index of end of word char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a811f328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</w>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_of_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea09005",
   "metadata": {},
   "source": [
    "### Pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec3aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word splits:  {('O', 'n', 'c', 'e', '</w>'): 2, ('u', 'p', 'o', 'n', '</w>'): 2, ('a', '</w>'): 24, ('t', 'i', 'm', 'e', ',', '</w>'): 2, ('i', 'n', '</w>'): 4, ('d', 'i', 's', 't', 'a', 'n', 't', '</w>'): 2, ('g', 'a', 'l', 'a', 'x', 'y', ',', '</w>'): 2, ('l', 'o', 'n', 'e', '</w>'): 2, ('e', 'x', 'p', 'l', 'o', 'r', 'e', 'r', '</w>'): 2, ('e', 'm', 'b', 'a', 'r', 'k', 'e', 'd', '</w>'): 2, ('o', 'n', '</w>'): 3, ('m', 'i', 's', 's', 'i', 'o', 'n', '</w>'): 2, ('t', 'o', '</w>'): 7, ('u', 'n', 'c', 'o', 'v', 'e', 'r', '</w>'): 2, ('t', 'h', 'e', '</w>'): 14, ('s', 'e', 'c', 'r', 'e', 't', 's', '</w>'): 2, ('o', 'f', '</w>'): 12, ('a', 'n', '</w>'): 5, ('a', 'n', 'c', 'i', 'e', 'n', 't', '</w>'): 2, ('c', 'i', 'v', 'i', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', '</w>'): 2, ('A', 's', '</w>'): 6, ('t', 'h', 'e', 'y', '</w>'): 11, ('d', 'e', 'c', 'o', 'd', 'e', 'd', '</w>'): 2, ('m', 'y', 's', 't', 'e', 'r', 'i', 'o', 'u', 's', '</w>'): 3, ('s', 'y', 'm', 'b', 'o', 'l', 's', ',', '</w>'): 2, ('t', 'r', 'u', 't', 'h', '</w>'): 2, ('a', 'b', 'o', 'u', 't', '</w>'): 3, ('t', 'h', 'e', 'i', 'r', '</w>'): 7, ('o', 'r', 'i', 'g', 'i', 'n', 's', '</w>'): 2, ('s', 'l', 'o', 'w', 'l', 'y', '</w>'): 2, ('u', 'n', 'f', 'o', 'l', 'd', 'e', 'd', '…', '</w>'): 2, ('I', 'n', '</w>'): 8, ('w', 'o', 'r', 'l', 'd', '</w>'): 2, ('w', 'h', 'e', 'r', 'e', '</w>'): 2, ('t', 'e', 'c', 'h', 'n', 'o', 'l', 'o', 'g', 'y', '</w>'): 2, ('a', 'n', 'd', '</w>'): 10, ('m', 'a', 'g', 'i', 'c', '</w>'): 2, ('c', 'o', 'e', 'x', 'i', 's', 't', 'e', 'd', ',', '</w>'): 2, ('y', 'o', 'u', 'n', 'g', '</w>'): 3, ('s', 'o', 'r', 'c', 'e', 'r', 'e', 'r', '</w>'): 2, ('d', 'i', 's', 'c', 'o', 'v', 'e', 'r', 'e', 'd', '</w>'): 4, ('h', 'i', 'd', 'd', 'e', 'n', '</w>'): 3, ('p', 'o', 'w', 'e', 'r', '</w>'): 3, ('w', 'i', 't', 'h', 'i', 'n', '</w>'): 2, ('t', 'h', 'e', 'm', 's', 'e', 'l', 'v', 'e', 's', '.', '</w>'): 2, ('W', 'i', 't', 'h', '</w>'): 2, ('h', 'e', 'l', 'p', '</w>'): 2, ('u', 'n', 'l', 'i', 'k', 'e', 'l', 'y', '</w>'): 2, ('a', 'l', 'l', 'y', ',', '</w>'): 2, ('s', 'e', 't', '</w>'): 3, ('o', 'u', 't', '</w>'): 3, ('m', 'a', 's', 't', 'e', 'r', '</w>'): 2, ('a', 'b', 'i', 'l', 'i', 't', 'i', 'e', 's', '</w>'): 2, ('c', 'o', 'n', 'f', 'r', 'o', 'n', 't', '</w>'): 2, ('i', 'm', 'p', 'e', 'n', 'd', 'i', 'n', 'g', '</w>'): 2, ('d', 'a', 'r', 'k', 'n', 'e', 's', 's', '</w>'): 2, ('t', 'h', 'a', 't', '</w>'): 4, ('t', 'h', 'r', 'e', 'a', 't', 'e', 'n', 'e', 'd', '</w>'): 2, ('r', 'e', 'a', 'l', 'm', '.', '</w>'): 2, ('p', 'o', 's', 't', '-', 'a', 'p', 'o', 'c', 'a', 'l', 'y', 'p', 't', 'i', 'c', '</w>'): 2, ('w', 'a', 's', 't', 'e', 'l', 'a', 'n', 'd', ',', '</w>'): 2, ('g', 'r', 'o', 'u', 'p', '</w>'): 3, ('s', 'u', 'r', 'v', 'i', 'v', 'o', 'r', 's', '</w>'): 2, ('b', 'a', 'n', 'd', 'e', 'd', '</w>'): 2, ('t', 'o', 'g', 'e', 't', 'h', 'e', 'r', '</w>'): 2, ('r', 'e', 'b', 'u', 'i', 'l', 'd', '</w>'): 2, ('s', 'o', 'c', 'i', 'e', 't', 'y', '.', '</w>'): 2, ('f', 'a', 'c', 'e', 'd', '</w>'): 2, ('e', 'x', 't', 'e', 'r', 'n', 'a', 'l', '</w>'): 2, ('t', 'h', 'r', 'e', 'a', 't', 's', '</w>'): 2, ('i', 'n', 't', 'e', 'r', 'n', 'a', 'l', '</w>'): 2, ('c', 'o', 'n', 'f', 'l', 'i', 'c', 't', 's', ',', '</w>'): 2, ('l', 'e', 'a', 'r', 'n', 'e', 'd', '</w>'): 2, ('t', 'r', 'u', 'e', '</w>'): 2, ('m', 'e', 'a', 'n', 'i', 'n', 'g', '</w>'): 2, ('c', 'o', 'm', 'm', 'u', 'n', 'i', 't', 'y', '</w>'): 2, ('r', 'e', 's', 'i', 'l', 'i', 'e', 'n', 'c', 'e', '.', '</w>'): 2, ('b', 'u', 's', 't', 'l', 'i', 'n', 'g', '</w>'): 2, ('m', 'e', 't', 'r', 'o', 'p', 'o', 'l', 'i', 's', ',', '</w>'): 2, ('d', 'e', 't', 'e', 'c', 't', 'i', 'v', 'e', '</w>'): 2, ('w', 'i', 't', 'h', '</w>'): 3, ('t', 'r', 'o', 'u', 'b', 'l', 'e', 'd', '</w>'): 2, ('p', 'a', 's', 't', '</w>'): 2, ('f', 'o', 'u', 'n', 'd', '</w>'): 2, ('t', 'h', 'e', 'm', 's', 'e', 'l', 'v', 'e', 's', '</w>'): 3, ('e', 'n', 't', 'a', 'n', 'g', 'l', 'e', 'd', '</w>'): 2, ('w', 'e', 'b', '</w>'): 2, ('c', 'r', 'i', 'm', 'e', '</w>'): 2, ('c', 'o', 'r', 'r', 'u', 'p', 't', 'i', 'o', 'n', '.', '</w>'): 2, ('n', 'a', 'v', 'i', 'g', 'a', 't', 'e', 'd', '</w>'): 2, ('s', 'h', 'a', 'd', 'o', 'w', 's', '</w>'): 2, ('c', 'i', 't', 'y', ',', '</w>'): 2, ('u', 'n', 'c', 'o', 'v', 'e', 'r', 'e', 'd', '</w>'): 2, ('c', 'o', 'n', 's', 'p', 'i', 'r', 'a', 'c', 'y', '</w>'): 2, ('r', 'e', 'a', 'c', 'h', 'e', 'd', '</w>'): 2, ('h', 'i', 'g', 'h', 'e', 's', 't', '</w>'): 2, ('e', 'c', 'h', 'e', 'l', 'o', 'n', 's', '</w>'): 2, ('p', 'o', 'w', 'e', 'r', '.', '</w>'): 2, ('s', 'm', 'a', 'l', 'l', '</w>'): 2, ('t', 'o', 'w', 'n', ',', '</w>'): 2, ('s', 't', 'r', 'a', 'n', 'g', 'e', 'r', '</w>'): 2, ('a', 'r', 'r', 'i', 'v', 'e', 'd', ',', '</w>'): 2, ('b', 'r', 'i', 'n', 'g', 'i', 'n', 'g', '</w>'): 2, ('t', 'h', 'e', 'm', '</w>'): 2, ('t', 'a', 'l', 'e', 's', '</w>'): 2, ('a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', '</w>'): 2, ('i', 'n', 't', 'r', 'i', 'g', 'u', 'e', '.', '</w>'): 2, ('t', 'o', 'w', 'n', 's', 'f', 'o', 'l', 'k', '</w>'): 2, ('b', 'e', 'c', 'a', 'm', 'e', '</w>'): 2, ('e', 'm', 'b', 'r', 'o', 'i', 'l', 'e', 'd', '</w>'): 2, ('s', 't', 'o', 'r', 'i', 'e', 's', ',', '</w>'): 2, ('t', 'r', 'u', 't', 'h', 's', '</w>'): 2, ('c', 'o', 'm', 'm', 'u', 'n', 'i', 't', 'y', '.', '</w>'): 2, ('f', 'a', 'n', 't', 'a', 's', 'y', '</w>'): 2, ('r', 'e', 'a', 'l', 'm', ',', '</w>'): 2, ('h', 'e', 'r', 'o', '</w>'): 2, ('q', 'u', 'e', 's', 't', '</w>'): 2, ('r', 'e', 't', 'r', 'i', 'e', 'v', 'e', '</w>'): 2, ('s', 't', 'o', 'l', 'e', 'n', '</w>'): 2, ('a', 'r', 't', 'i', 'f', 'a', 'c', 't', '</w>'): 2, ('h', 'e', 'l', 'd', '</w>'): 2, ('k', 'e', 'y', '</w>'): 2, ('r', 'e', 's', 't', 'o', 'r', 'i', 'n', 'g', '</w>'): 2, ('b', 'a', 'l', 'a', 'n', 'c', 'e', '</w>'): 2, ('w', 'o', 'r', 'l', 'd', '.', '</w>'): 2, ('A', 'l', 'o', 'n', 'g', '</w>'): 2, ('w', 'a', 'y', ',', '</w>'): 2, ('e', 'n', 'c', 'o', 'u', 'n', 't', 'e', 'r', 'e', 'd', '</w>'): 2, ('m', 'y', 't', 'h', 'i', 'c', 'a', 'l', '</w>'): 2, ('c', 'r', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', '</w>'): 2, ('t', 'r', 'e', 'a', 'c', 'h', 'e', 'r', 'o', 'u', 's', '</w>'): 2, ('l', 'a', 'n', 'd', 's', 'c', 'a', 'p', 'e', 's', ',', '</w>'): 2, ('u', 'n', 'e', 'x', 'p', 'e', 'c', 't', 'e', 'd', '</w>'): 2, ('a', 'l', 'l', 'i', 'e', 's', '.', '</w>'): 2, ('d', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', '</w>'): 2, ('f', 'u', 't', 'u', 'r', 'e', ',', '</w>'): 2, ('r', 'e', 'b', 'e', 'l', 's', '</w>'): 2, ('f', 'o', 'u', 'g', 'h', 't', '</w>'): 2, ('a', 'g', 'a', 'i', 'n', 's', 't', '</w>'): 2, ('o', 'p', 'p', 'r', 'e', 's', 's', 'i', 'v', 'e', '</w>'): 2, ('r', 'e', 'g', 'i', 'm', 'e', '.', '</w>'): 2, ('r', 'i', 's', 'k', 'e', 'd', '</w>'): 2, ('e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', '</w>'): 2, ('f', 'o', 'r', '</w>'): 2, ('f', 'r', 'e', 'e', 'd', 'o', 'm', ',', '</w>'): 2, ('h', 'o', 'p', 'e', '</w>'): 2, ('s', 't', 'r', 'e', 'n', 'g', 't', 'h', '</w>'): 2, ('h', 'u', 'm', 'a', 'n', '</w>'): 2, ('s', 'p', 'i', 'r', 'i', 't', '.', '</w>'): 2, ('h', 'i', 's', 't', 'o', 'r', 'i', 'c', 'a', 'l', '</w>'): 2, ('s', 'e', 't', 't', 'i', 'n', 'g', '.', '</w>'): 2}\n"
     ]
    }
   ],
   "source": [
    "word_splits = {}\n",
    "for sentence in corpus:\n",
    "    # Split the sentence into words\n",
    "   words = sentence.split(\" \")\n",
    "   for word in words:\n",
    "      if word:\n",
    "         char_list = list(word) + [end_of_word]\n",
    "\n",
    "         word_tuple = tuple(char_list)\n",
    "         if word_tuple not in word_splits:\n",
    "            word_splits[word_tuple] = 1\n",
    "         word_splits[word_tuple] += 1  # count frequency of word\n",
    "print(\"word splits: \", word_splits)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c530260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_pair_stats(splits): # get the frequency of pairs of characters\n",
    "    pair_counts = collections.defaultdict(int)\n",
    "    #print(\"splits: \", splits)\n",
    "    for word, freq in splits.items():\n",
    "        #print(\"word: \", word)       # LOGGING\n",
    "        #print(\"freq: \", freq)  \n",
    "        word = list(word) \n",
    "        #print(word)    # LOGGING\n",
    "        for i in range(len(word), -1):\n",
    "            #print(\"i: \", i)           # LOGGING\n",
    "            \n",
    "            pair = (word[i], word[i+1])\n",
    "            pair_counts[pair] += freq\n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7770a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pair_stats(word_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d22ca4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = get_pair_stats(word_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3500035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair counts:  defaultdict(<class 'int'>, {})\n"
     ]
    }
   ],
   "source": [
    "print(\"pair counts: \", pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a67bbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(pairs_to_merge, splits):\n",
    "    # Merge the most frequent pair in the splits\n",
    "    new_splits = {}\n",
    "    for word, freq in splits.items():\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            pair = (word[i], word[i+1])\n",
    "            if pair == pairs_to_merge:\n",
    "                new_word.append(word[i] + word[i+1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        if new_word not in new_splits:\n",
    "            new_splits[new_word] = 0\n",
    "        new_splits[new_word] += freq\n",
    "    return new_splits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552992df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
